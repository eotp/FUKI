{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb03f365-33c3-4692-834b-cc30169a6018",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec1c064-c66a-4314-bd69-aa4a4bfe51f1",
   "metadata": {},
   "source": [
    "## Organization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f60d15-ab71-469f-9b61-25ba5b567316",
   "metadata": {},
   "source": [
    "Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c937cc5-b73d-45f6-acf4-8913aab56ecd",
   "metadata": {},
   "source": [
    "How we connect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff2711e-dff2-4fa1-b2d4-6d481bac6a5d",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb9164d-caa1-4523-985d-8d2b8680f919",
   "metadata": {},
   "source": [
    "different IDE´s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b112a4-77cf-408f-9f12-112a45e7b6d2",
   "metadata": {},
   "source": [
    "environment - libmamba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7241565-4afb-4a4c-b848-b9674a62990f",
   "metadata": {},
   "source": [
    "Download Minianaconda **[here](https://www.anaconda.com/docs/getting-started/miniconda/install#quickstart-install-instructions)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb80e27-cf26-4b3e-992e-dd948892053d",
   "metadata": {},
   "source": [
    "* conda create --name FUKI python=3.10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fbfc12-3f3e-4f7c-b710-9869d0373e45",
   "metadata": {},
   "source": [
    "* conda activate FUKI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a675c0f4-b8e4-4bd0-a6b3-d1ede788d7cf",
   "metadata": {},
   "source": [
    "* conda install jupyterlab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb997561-5bc7-4746-89f3-6e90ff4c10fc",
   "metadata": {},
   "source": [
    "* conda install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58393c10-c73f-4632-807f-31026d6f7aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### * conda install conda-forge::tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc7c934-6ab9-43ff-a1f5-639408da826f",
   "metadata": {},
   "source": [
    "* conda install anaconda::scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681540de-3161-42be-972e-576785fc2734",
   "metadata": {},
   "source": [
    "* conda install statsmodels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d0c25f-0c96-4295-be63-b5f8cb464959",
   "metadata": {},
   "source": [
    "Test computer - GPU support ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9079a1b-580a-4abf-b43f-32295c703cfa",
   "metadata": {},
   "source": [
    "## Python Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3bd8c7-eecc-4925-809d-5de5a61f486c",
   "metadata": {},
   "source": [
    "Python 101"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18c80f9-2f53-4233-a01e-cbd0422e150a",
   "metadata": {},
   "source": [
    "What is machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4fdd0c-149a-40bf-a688-24d598d68c6c",
   "metadata": {},
   "source": [
    "What is Keras?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceccdc04-458c-45b1-b57f-04452f17b49e",
   "metadata": {},
   "source": [
    "Keras was created by **[Francois Chollet](https://en.wikipedia.org/wiki/François_Chollet)** as  a high-level API between for the low level backend libraries Thenao, CNTK and TensorFlow. It was later on absorbed into TensorFlow and now serves as an easy access to the TensorFlow library. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95830e3-ddb0-4382-98aa-c36ebb70ce8d",
   "metadata": {},
   "source": [
    "Installing Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b0355f-9757-4056-83e2-e7cb1a85e58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set environment variable OMP_NUM_THREADS\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b88de8-fece-41f4-8ffc-21b7d1701033",
   "metadata": {},
   "source": [
    "# Introduction and overview of machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707512b1-58a6-4512-99a1-b45e829a1510",
   "metadata": {},
   "source": [
    "**Machine learning** is a branch of artificial intelligence in which information is obtained by using algorithms to analyze data. For example, speech recognition is an algorithmic conversion of acoustic information into text. But how can information be obtained using algorithms and what different approaches are used in machine learning? We want to take a closer look at this in this introductory session.\n",
    "\n",
    "In this course we aim at providing you with the necessary mathematical knowledge, methods and programming skills to conduct your own machine learning projects. Let´s start by familiarizing ourselves with some of the fundamental concepts of machine learning so we can later on put the methods we have already learned into a broader context by apllying them to solve problems. The various machine learning algorithms will be briefly characterized and explained in more detail in the following sessions. \n",
    "\n",
    "Let's start by asking ourselves some questions ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4953ac-a545-45ed-83f2-d1b205dd4a5a",
   "metadata": {
    "citation-manager": {
     "citations": {
      "4k3ee": [
       {
        "id": "16738657/N6ILIT6K",
        "source": "zotero"
       }
      ]
     }
    }
   },
   "source": [
    "## What is machine learning?\n",
    "\n",
    "**[Machine Learning](https://en.wikipedia.org/wiki/Machine_learning)** (ML) is a branch of artificial intelligence that gives computers the ability to learn from experience and automate tasks without having to be explicitly programmed. The core idea behind ML is that computers can analyze data, recognize patterns and make predictions or decisions based on them.\n",
    "\n",
    "In contrast to previous approaches, which were based on expert and rule-based systems and were heavily dependent on domain knowledge, machine learning aims to extract generic regularities from high-dimensional data sets. This is achieved by training different models (algorithms) with suitable data and adjusting the **model-specific parameters** to achieve the highest possible **prediction accuracy** measured against an **evaluation metric**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f89d818-143e-4c78-b159-d87ec44debdb",
   "metadata": {
    "citation-manager": {
     "citations": {
      "3bnfd": [
       {
        "id": "16738657/N6ILIT6K",
        "source": "zotero"
       }
      ]
     }
    },
    "tags": []
   },
   "source": [
    "## How does the machine learning process work?\n",
    "\n",
    "We have already seen some examples of the machine learning process during the course, but we will summarize these points again to illustrate the system. \n",
    "\n",
    "In the following diagram, you can see a schematic overview of the individual steps of the machine learning process:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3de4306-911d-415b-bcdf-ee2f3a1f9976",
   "metadata": {},
   "source": [
    "<img src=\"./images/ML_scheme.png\" alt=\"drawing\" width=\"80%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f14fec4-db41-48f2-a9ee-307d5b21db98",
   "metadata": {},
   "source": [
    "## Machine learning process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db108919-e5a0-45c2-86a0-05d7b78dd177",
   "metadata": {},
   "source": [
    "Let's take a look at the individual steps of the **machine learning process** using an example from **polynomial regression**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661d11a7-88c4-4241-b6d6-794351944b61",
   "metadata": {},
   "source": [
    "- **Data collection:**\n",
    "\n",
    "Initial data is required for the training of **machine learning algorithms**. Which data must be collected and to what extent depends on the problem on the one hand and on domain knowledge on the other by excluding irrelevant features in advance. It should be mentioned that machine learning, in contrast to human pattern recognition, can achieve good results with high-dimensional data (many feature vectors) and therefore a hasty exclusion of features in some cases leads to poorer predictions. However, the collection of data can be time-consuming and cost-intensive, so a middle way often makes sense."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e9d113-fa8d-479f-80b0-15ac3f517e8c",
   "metadata": {},
   "source": [
    "To get an impression of the general procedures, let's look at an example data set. We generate $350$ data points from a **[continuous uniform distribution](https://en.wikipedia.org/wiki/Continuous_uniform_distribution)** in the interval $[-5 , 5]$ using the `NumPy` function `random.uniform()` for the function $f(x) = x^2$ and add random noise with the function `random.normal()`. In addition, we insert `NaN` values at the beginning of the data set to practise dealing with missing data, save the result in `data` and create a `Pandas` dataframe from it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef84cf41-8fa1-4b66-b598-7c448e179990",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "seed = 124\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Number of data points\n",
    "n = 350\n",
    "\n",
    "# Generate n random x values in the range from -5 to 5\n",
    "x = np.random.uniform(-5, 5, n - 1)\n",
    "\n",
    "# Generate the corresponding y-values with the function x^3 and random noise\n",
    "y = x**3 + 20 * np.random.normal(0, 0.125, n - 1)\n",
    "\n",
    "# Add a NaN value at the beginning of the x and y arrays\n",
    "x = np.insert(x, 0, np.nan)\n",
    "y = np.insert(y, 0, np.nan)\n",
    "\n",
    "data_df = pd.DataFrame({\"x\": x, \"y\": y})\n",
    "\n",
    "data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5546837-a4b6-4238-b988-2ad8915b9e68",
   "metadata": {},
   "source": [
    "As we can see, the data set contains zero values, which are also referred to as 'NaN' values. The handling of missing or incorrect data is dealt with in the next step during data cleansing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d73350-0572-4920-9dd2-3f220e58a183",
   "metadata": {},
   "source": [
    "- **Data cleansing:**\n",
    "\n",
    "The purpose of data cleansing is to check the validity of the training data. Missing entries, incorrect entries, outliers and duplicates must either be removed or replaced by **[imputation methods](https://en.wikipedia.org/wiki/Imputation_(statistics))**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f41e24-9056-4901-b506-fb73841518f8",
   "metadata": {},
   "source": [
    "In the case of our example data set, we remove the missing data using the `dropna()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8255db3-35a1-4ff1-8697-395b592bd629",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_df_clean = data_df.copy()\n",
    "data_df_clean = data_df_clean.dropna()\n",
    "data_df_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e77a1c6-5747-42f3-afbb-0f758579595a",
   "metadata": {},
   "source": [
    "In the next step, we turn to data exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60a690c-70e4-4b7d-849c-edf7aa87268c",
   "metadata": {},
   "source": [
    "- **Data exploration:** \n",
    "\n",
    "Statistical analysis and visualization of data in the context of **[explorative data analysis](https://en.wikipedia.org/wiki/Exploratory_data_analysis) (EDA)** makes it possible to identify patterns in the data in advance and to examine relationships between individual features. This makes it possible to recognize potential problems in the evaluation at an early stage or to carry out preliminary work for feature engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e3ef9e-417d-40a0-916f-a9551c007986",
   "metadata": {},
   "source": [
    "Let's carry out an exploratory data analysis for the sample data as an example:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabbdd14-5149-4d71-8180-c9a948c648cc",
   "metadata": {},
   "source": [
    "First, we assign the dependent variable or **target variable** and the independent variable or **feature variable**. In our example, these simply correspond to the $y$ and $X$ values respectively. In other problems, however, it may be necessary to consider many features for the prediction of the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff2fec5-6cdd-4ec1-81ae-3e1a0eaa64e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_df_clean.x\n",
    "y = data_df_clean.y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bfec14-3dc4-4c24-a266-f2e5d7543fa1",
   "metadata": {},
   "source": [
    "In a first step, we can use the methods `info()` and `describe()` to calculate the characteristic statistical parameters of the data. These methods return the data types contained in the data set as well as the number, mean value, standard deviation, minimum, first, second and third quantile and the maximum of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485fc3e9-45bc-406e-ae3c-d27750cafeae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(data_df_clean.info())\n",
    "print(data_df_clean.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea387e5-683b-4fe4-b6ca-07bac194c999",
   "metadata": {},
   "source": [
    "We can conclude from this that the data set contains numerical data of the `float64` data type in equal numbers. The mean value, the standard deviation, the minimum, the first, second and third quantile as well as the maximum can give us information about the central tendency, the dispersion and the distribution form of the data. For example, the $y$ values have a significantly higher scatter around the mean due to their higher standard deviation, which indicates a non-linear relationship in relation to $y$ as the dependent variable of $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471c802a-e1b6-46f7-b7ee-11c3cc0bb052",
   "metadata": {},
   "source": [
    "When exploring data, it can also be useful to visualize the it in order to gain an overview of the distribution of the data. We present three common EDA visualization methods below.\n",
    "\n",
    "\n",
    "The representation as **[scatter plot](https://en.wikipedia.org/wiki/Scatter_plot)**  enables an assessment of the dependency structure in the data by plotting the values of the data set in pairs as points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6671ff11-c2f5-4eec-924d-cd54b971cc97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "_ = plt.scatter(X, y, s=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447ceaa7-8dc8-46c7-b13a-780039a228c2",
   "metadata": {},
   "source": [
    "Another frequently used form of representation is the **[histogram](https://en.wikipedia.org/wiki/Histogram)**, which provides an overview of the frequency distribution of the data. The data is divided into classes (*bins*), whereby the number of data in the respective class corresponds to the height:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c088e2-22d5-4ed7-bc41-3c2ca87da514",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.hist(data_df_clean.y, bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577d607c-3ef1-490a-9233-20dda7e1a896",
   "metadata": {},
   "source": [
    "When displaying the data in a **[boxplot](https://en.wikipedia.org/wiki/Box_plot)**, the median (yellow line), the first and third quartiles (black rectangle) and the \"antennae\" or whiskers, which correspond to $1.5$ times the interquartile range, are plotted. Points outside the whiskers are plotted as black circles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0836b1-f558-4cd2-af6e-039fbc6c8ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.boxplot([data_df_clean.x, data_df_clean.y], labels=[\"$X$-Werte\", \"$y$-Werte\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22021de-dfea-4de4-846b-f11882610d68",
   "metadata": {
    "citation-manager": {
     "citations": {
      "4kev9": [
       {
        "id": "16738657/WPIDC5X6",
        "source": "zotero"
       }
      ]
     }
    }
   },
   "source": [
    "More detailed descriptions of the use of diagrams for exploratory data analysis can be found in <cite id=\"4kev9\"><a href=\"#zotero%7C16738657%2FWPIDC5X6\">(Bruce et al., 2021)</a></cite> (pp. 1-47)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1686fbb4-81eb-4b4b-9970-1cbab930c012",
   "metadata": {},
   "source": [
    "- **Train-Test-Split:**\n",
    "\n",
    "As already discussed in another section, it makes sense to split the available data into a training, test and validation data set in order to generalize the model. While the model is trained on the training data, the model parameters are tuned by evaluating the predictions on the validation data set. The final evaluation of the model is then carried out using the test data. This splitting ensures a realistic estimation of the model's performance on new, previously unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4b9e56-ba28-4641-832a-93082ddffb87",
   "metadata": {},
   "source": [
    "We can use the `train_test_split()` function in `scikit-learn` to split the data into training and test data sets. To create an additional validation dataset, we can use `train_test_split()` one more time to split the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e94d16-57bd-4a82-9236-2f394ea46430",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=21\n",
    ")\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42\n",
    ")\n",
    "\n",
    "plt.scatter(X_val, y_val, label=r\"10 % validation data\", s=5, color=\"red\")\n",
    "plt.scatter(X_test, y_test, label=r\"10 % test data\", s=5, color=\"blue\")\n",
    "plt.scatter(\n",
    "    X_train, y_train, label=r\"80 % training data\", s=5, alpha=0.25, color=\"green\"\n",
    ")\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229849c4-fddb-4d96-8e72-7395d6b8ebfa",
   "metadata": {
    "citation-manager": {
     "citations": {
      "g1tw5": [
       {
        "id": "16738657/P59K4ZW6",
        "source": "zotero"
       }
      ]
     }
    }
   },
   "source": [
    "- **Model selection:** \n",
    "\n",
    "The choice of model used depends on the problem being addressed (e.g. **classification**, **regression** or **clustering**) as well as on factors such as the size of the data set and the specific advantages and disadvantages of the models, which we will discuss in more detail <cite id=\"g1tw5\"><a href=\"#zotero%7C16738657%2FP59K4ZW6\">(Géron, 2020)</a></cite> (pp. 31-33). Other factors include the type of data used (**categorical**, **nominal**, **ordinal**, **numeric**), **over** or **underfitting**, the **hyperparameters** used in the model and, finally, the **interpretability of the model**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29dc8ee-dcd5-4a31-a311-1df6337c6275",
   "metadata": {},
   "source": [
    "In our example, we assume a polynomial function through **data exploration** in order to describe the distribution of the data points. \n",
    "\n",
    "We therefore use the following approach:\n",
    "\n",
    "$$f(x) = a_0 + a_1 x + a_2 x^2 + \\cdots + a_n x^n $$\n",
    "\n",
    "where $a_0,a_1,\\cdots,a_n$ are the coefficients and $n$ is the order of the polynomial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f539d06-01d2-49ac-976a-a518a492685a",
   "metadata": {},
   "source": [
    "- **Feature Engineering** \n",
    "\n",
    "**Feature engineering** is about creating new features and transforming existing data to enable better model prediction results. Examples include scaling data, creating polynomial features or merging highly correlated features for dimensionality reduction. An important application of feature engineering is, for example, the **one-hot encoding** of categorical data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c95cb8-70b4-4f0e-af9f-20943d9298fb",
   "metadata": {},
   "source": [
    "With reference to our example, we add $n$ powers of the $x$ values (i.e. $x^0, x^1, \\cdots , x^n$) using the function `PolynomialFeatures` and the argument `degree=n`. If there are several independent variables $[a,b]$, all combinations of the features $[a,b,ab,a^2,b^2]$ must also be taken into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ee2629-92f1-430e-b5d2-cf4a0629a1a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Create polynomial features for training data\n",
    "poly_features = PolynomialFeatures(degree=5)\n",
    "X_poly_train = poly_features.fit_transform(X_train.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d5b739-8780-4614-8568-f43e974c78ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create polynomial regression model\n",
    "poly_mod = LinearRegression()\n",
    "poly_mod.fit(X_poly_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b4fee2-68ff-4296-b8d5-dcea6ae7ca99",
   "metadata": {},
   "source": [
    "We create a regression line with predictions of our model `poly_mod` for $1.000$ uniformly distributed points in the interval $[-10 , 10]$ and plot the regression line with the original data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5629d3-c379-4389-923b-5a973222b04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create x-values for regression line\n",
    "x_reg = np.linspace(-5, 5, 1000)\n",
    "\n",
    "# Create x_reg_poly\n",
    "x_reg_poly = poly_features.fit_transform(x_reg.reshape(-1, 1))\n",
    "\n",
    "# Make predictions for x_reg\n",
    "pred_reg = poly_mod.predict(x_reg_poly)\n",
    "\n",
    "plt.plot(\n",
    "    x_reg,\n",
    "    pred_reg,\n",
    "    color=\"red\",\n",
    "    linestyle=\"dashed\",\n",
    "    linewidth=1,\n",
    "    label=\"Regression line\",\n",
    ")\n",
    "plt.legend()\n",
    "plt.title(\"Polynomial Regression\")\n",
    "\n",
    "_ = plt.scatter(X, y, s=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371d2f81-12cf-4d29-a7d1-4eaa4945623e",
   "metadata": {},
   "source": [
    "- **Model evaluation:**\n",
    "\n",
    "In this step, the quality of the model's predictions is checked. Depending on the problem and the model used, there are different metrics that can be applied. Examples include the **[RMSE](https://en.wikipedia.org/wiki/Root-mean-square_deviation)**, the **[MSE](https://en.wikipedia.org/wiki/Mean_squared_error)**, the **[coefficient of determination](https://en.wikipedia.org/wiki/Coefficient_of_determination)** and the **[accuracy](https://en.wikipedia.org/wiki/Accuracy_and_precision)**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c81f937-ded5-44b6-9b56-dcec5af727eb",
   "metadata": {},
   "source": [
    "We will discuss the different metrics for different machine learning algorithms in detail later. For the model evaluation in our example, we opt for the **MSE** to evaluate the predictions for the validation data `X_val` of our model and the actual values `y_val`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed04312f-ae5e-4c6a-a4b9-a5671d19e29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Create PolynomialFeatures (degree=5)\n",
    "poly_features = PolynomialFeatures(degree=5)\n",
    "\n",
    "# Create polynomial features for validation data\n",
    "x_val_poly = poly_features.fit_transform(X_val.values.reshape(-1, 1))\n",
    "\n",
    "# Create polynomial regression model\n",
    "poly_mod = LinearRegression()\n",
    "poly_mod.fit(X_poly_train, y_train)\n",
    "\n",
    "y_preds = poly_mod.predict(x_val_poly)\n",
    "\n",
    "mse = mean_squared_error(y_val, y_preds)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE) for polynomial of order 5: {mse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77bd26c-e704-4652-b286-370fe38ac932",
   "metadata": {},
   "source": [
    "We can output the parameters of the model with the attributes `intercept_` (the constant term $a_0$, also called intercept or bias and `coef_` (the coefficients of the polynomial $a_1, a_2, \\cdots , a_n$). Note that the first value in `coef_` serves as a placeholder for the intercept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976b6eda-2dbb-429c-8c49-38143b48281a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Polynomial of order 5\")\n",
    "print(\"Bias (Intercept):\", poly_mod.intercept_)\n",
    "print(\"Weights (coefficients):\", poly_mod.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a6b5a2-46aa-4694-9961-566a5309c543",
   "metadata": {
    "citation-manager": {
     "citations": {
      "ticvo": [
       {
        "id": "16738657/P59K4ZW6",
        "source": "zotero"
       }
      ]
     }
    }
   },
   "source": [
    "- **Hyperparameter tuning:**\n",
    "\n",
    "Various methods such as **grid search**, **random search** and **cross-validation** are used to optimize **hyperparameters** <cite id=\"ticvo\"><a href=\"#zotero%7C16738657%2FP59K4ZW6\">(Géron, 2020)</a></cite> (pp. 78-82), i.e. parameters that are included in the model before the data is trained. With **Grid Search** the parameter space is systematically scanned, while with **Random search** certain value ranges of the parameter space are selected at random. The different combinations of parameters are evaluated, for example by minimizing a **Loss** function, and the ideal combination is selected. **Cross-validation** is used to use parts of the training dataset to validate the model and only in the last step to divide the dataset into test and training datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8773419-36b9-4051-bd3f-6832c04785a2",
   "metadata": {},
   "source": [
    "We will discuss `GridSearchCV` and its possible uses for hyperparameter tuning later in this chapter. For our example, we use a `for` loop to evaluate the model for different values of the hyperparameter, which in this case corresponds to the order of the polynomial features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ca9909-9b5b-44c6-af02-40cd8920a840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list for the individual models, 0 serves as placeholder\n",
    "modelle = [0]\n",
    "\n",
    "for i in range(1, 6):\n",
    "    # Create PolynomialFeatures (degree=i)\n",
    "    poly_features = PolynomialFeatures(degree=i)\n",
    "\n",
    "    # Create x_test_poly (degree=i)\n",
    "    X_val_poly = poly_features.fit_transform(X_val.values.reshape(-1, 1))\n",
    "\n",
    "    # Create polynomial features for training data\n",
    "    X_poly_train = poly_features.fit_transform(X_train.values.reshape(-1, 1))\n",
    "\n",
    "    # Create polynomial regression model\n",
    "    poly_mod = LinearRegression()\n",
    "    poly_mod.fit(X_poly_train, y_train)\n",
    "\n",
    "    y_preds = poly_mod.predict(X_val_poly)\n",
    "\n",
    "    mse = mean_squared_error(y_val, y_preds)\n",
    "\n",
    "    # Add the model to the list\n",
    "    modelle.append(poly_mod)\n",
    "\n",
    "    print(f\"Mean Squared Error (MSE) of the model of the order: {i} : {mse:.2f}\")\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11766e2-5950-4e99-8147-e53e5cfdc33c",
   "metadata": {},
   "source": [
    "We recognize that the best evaluated model in this case is the one with third-order polynomial features. However, since models three through five are close to each other in their evaluation metric, we compare their performance in making predictions on the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed12367-e7e3-4a06-9232-56e22bdae8ea",
   "metadata": {},
   "source": [
    "- **Model validation:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324cf602-a0da-41db-936b-60dcb5be61ea",
   "metadata": {},
   "source": [
    "We evaluate the generalization capabilities of our models by making predictions on the previously unused test data and apply the same evaluation metric (**MSE**) as on the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bdfe3c-7ddc-4034-a98b-8b4841d42a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3, 6):\n",
    "    # Create PolynomialFeatures (degree=i)\n",
    "    poly_features = PolynomialFeatures(degree=i)\n",
    "\n",
    "    # Create x_test_poly (degree=i)\n",
    "    X_test_poly = poly_features.fit_transform(X_test.values.reshape(-1, 1))\n",
    "    y_preds = modelle[i].predict(X_test_poly)\n",
    "\n",
    "    mse = mean_squared_error(y_test, y_preds)\n",
    "\n",
    "    print(f\"Mean Squared Error (MSE) of the model of the order: {i} : {mse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e42b94-6255-4a6b-ac70-cf5e96031a89",
   "metadata": {},
   "source": [
    "Our evaluation metric yields a similar value compared to the validation set and rates the third-order model best. If a model scores significantly better on the validation set, this may indicate **[overfitting](https://en.wikipedia.org/wiki/Overfitting)**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8991081-21f5-4ded-a6c2-75e03c49a287",
   "metadata": {},
   "source": [
    "In the last step, we display the three models and the underlying function without noise ($f(x) = x^3$) in the range $[-20, 20]$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3307fbc-994a-47a9-b54e-4d0b3e4f943f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred = np.linspace(-20, 20, 1000)\n",
    "colors = [\"red\", \"blue\", \"yellow\"]\n",
    "labels = [\"3rd order polynomial\", \"4th order polynomial\", \"5th order polynomial\"]\n",
    "for i in range(3, 6):\n",
    "    # Create PolynomialFeatures (degree=i)\n",
    "    poly_features = PolynomialFeatures(degree=i)\n",
    "\n",
    "    # Create x_test_poly (degree=i)\n",
    "    X_pred_poly = poly_features.fit_transform(X_pred.reshape(-1, 1))\n",
    "    y_preds = modelle[i].predict(X_pred_poly)\n",
    "    plt.plot(X_pred, y_preds, color=colors[i - 3], label=labels[i - 3])\n",
    "plt.plot(X_pred, X_pred**3, color=\"k\", linestyle=\"--\", label=\"Function without noise\")\n",
    "plt.grid()\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa857374-bdb8-42e4-934a-3cf91ecc56a6",
   "metadata": {
    "citation-manager": {
     "citations": {
      "fs80l": [
       {
        "id": "16738657/GXMJ3Q3D",
        "source": "zotero"
       }
      ],
      "yklgc": [
       {
        "id": "16738657/N6ILIT6K",
        "source": "zotero"
       }
      ]
     }
    },
    "tags": []
   },
   "source": [
    "## What types of machine learning are there?\n",
    "\n",
    "There are three main types of machine learning <cite id=\"yklgc\"><a href=\"#zotero%7C16738657%2FN6ILIT6K\">(Matzka, 2021)</a></cite> (pp. 10-14), <cite id=\"fs80l\"><a href=\"#zotero%7C16738657%2FGXMJ3Q3D\">(Awad &#38; Khanna, 2015)</a></cite> (pp. 5-9):\n",
    "**[supervised learning](https://en.wikipedia.org/wiki/Supervised_learning)**, **[unsupervised learning](https://en.wikipedia.org/wiki/Machine_learning#Unsupervised_learning)** and **[reinforcement learning](https://en.wikipedia.org/wiki/Reinforcement_learning)**. There are also other hybrid forms such as partially supervised learning or active learning, which will not be discussed in this course.\n",
    "\n",
    "In **supervised learning**, input data and the corresponding output data (labels) are used, i.e. data for which the correct predictions are known, in order to train the model. The goal of supervised learning is to extrapolate generalized rules through training in order to make correct predictions on unknown data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8f9706-6140-4704-93fb-b11576f9991b",
   "metadata": {},
   "source": [
    "**Unsupervised learning** pursues the goal of extracting significant correlations from the data without the use of labels. The computer learns to independently identify patterns or structures in data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d38359e-be53-4c6b-8557-ccca360fca74",
   "metadata": {},
   "source": [
    "The approach of **reinforced learning** is to let the computer (agent) learn through interaction with an environment by trial and error, where each interaction leads to a new state of the environment that is evaluated by a reward function. In order not to go beyond the scope of this chapter, we cannot discuss reinforcement learning in detail here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6af5dfe-fc54-45e1-aec6-4f095e8a36a5",
   "metadata": {},
   "source": [
    "It should be mentioned that **[Deep Learning](https://en.wikipedia.org/wiki/Deep_learning)** based on **[artificial neural networks](https://en.wikipedia.org/wiki/Neural_network_(machine_learning))** can be regarded as another method of machine learning. We will look at neural networks and deep learning in more detail in the chapter Deep Learning with `Keras`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0cfb0e-5949-4e03-b1d7-cebc351ead0b",
   "metadata": {},
   "source": [
    "The following figure shows a schematic breakdown of the different types of machine learning and their main applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc87c85-e0d4-401c-b8a8-584aecdffb23",
   "metadata": {},
   "source": [
    "<img src=\"./images/ML3_engl.png\" alt=\"drawing\" width=\"80%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9aab68d-274c-4ea9-8507-072325d6714d",
   "metadata": {},
   "source": [
    "## Overview of evaluation metrics:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea877a4-e00a-4413-be6e-2da033661b27",
   "metadata": {},
   "source": [
    "Before we go into the individual forms of machine learning in more detail, this summary provides a brief overview of metrics that are used for different types of machine learning.\n",
    "\n",
    "The importance of these evaluation metrics lies in the fact that by choosing metrics suitable for the problem under consideration, the performance of different models and hyperparameters can be assessed and improvements made to the model.\n",
    "\n",
    "The metrics are categorized into **classification**, **regression** and **clustering** based on the applications in which they are used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bb4d57-f53e-4d3f-b301-e637af782af7",
   "metadata": {
    "citation-manager": {
     "citations": {
      "oo4dr": [
       {
        "id": "16738657/N6ILIT6K",
        "source": "zotero"
       }
      ]
     }
    }
   },
   "source": [
    "**Metrics for classification:**\n",
    "\n",
    "<cite id=\"oo4dr\"><a href=\"#zotero%7C16738657%2FN6ILIT6K\">(Matzka, 2021)</a></cite> (pp. 140-156)\n",
    "\n",
    "**[Accuracy](https://en.wikipedia.org/wiki/Accuracy_and_precision)**: Proportion of correct predictions of class assignments, suitable for balanced class ratios.\n",
    "    \n",
    "$$\\text{accuracy} = \\frac{TP + TN}{TP + TN + FN + FP} $$\n",
    "\n",
    "In the binary case, $TP$ (True Positive) are the correct class assignments of the model to class $0$, $TN$ (True Negative) are the correct class assignments of the model to class $1$, $FP$ are the incorrect class assignments of the model to class $0$ and $FN$ are the incorrect class assignments of the model to class $1$. $FP$ therefore corresponds to an error of the $1$th type ($\\alpha$ error) and $FN$ to an error of the $2$th type ($\\beta$ error)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d324cf-2551-432a-96e7-decd01e01a9d",
   "metadata": {},
   "source": [
    "**[Precision](https://en.wikipedia.org/wiki/Precision_and_recall)**:\n",
    "\n",
    "Proportion of correctly positively predicted instances out of all positively predicted instances, helpful when it is important to avoid false positives ($FP$).\n",
    "    \n",
    "$$\\text{precision} = \\frac{TP}{TP + FP} $$\n",
    "\n",
    "**[Recall (Sensitivity, True Positive Rate)](https://en.wikipedia.org/wiki/Precision_and_recall)**: \n",
    "\n",
    "Proportion of correctly positive predicted class assignments out of all true positive cases, important to avoid missing positive class assignments.\n",
    "    \n",
    "$$\\text{Recall} = \\frac{TP}{TP + FN} $$\n",
    "\n",
    "**[F1 score](https://en.wikipedia.org/wiki/Precision_and_recall#F-measure)**:\n",
    "\n",
    "The harmonic mean between precision and recall, suitable for unbalanced classes.\n",
    "    \n",
    "$$\\text{F-Score} = \\frac{(\\beta^2 +1)\\cdot P \\cdot R}{\\beta^2 \\cdot P + R} \\rightarrow \\text{F1-Score} = \\frac{2\\cdot P \\cdot R}{ P + R} = \\frac{2}{P^{-1} + R^{-1}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782f49d9-3284-470c-9bee-d71ec133b143",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "# Create artificial data\n",
    "X, y = make_classification(n_samples=100000, n_features=20, random_state=42)\n",
    "\n",
    "# Divide the data into training and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Create a logistic regression model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_scores = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate ROC curve and AUC\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_scores)\n",
    "roc_auc = roc_auc_score(y_test, y_scores)\n",
    "\n",
    "# Create the ROC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color=\"blue\", lw=2, label=\"ROC curve (AUC = %0.2f)\" % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color=\"gray\", linestyle=\"--\")\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Receiver Operating Characteristic (ROC) Curve\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990fd8cf-d235-4487-a84c-a64fb9d77739",
   "metadata": {
    "citation-manager": {
     "citations": {
      "c62fn": [
       {
        "id": "16738657/WPIDC5X6",
        "source": "zotero"
       }
      ],
      "dejcm": [
       {
        "id": "16738657/WPIDC5X6",
        "source": "zotero"
       }
      ],
      "ph4pt": [
       {
        "id": "16738657/N6ILIT6K",
        "source": "zotero"
       }
      ]
     }
    }
   },
   "source": [
    "**[ROC (Receiver Operating Characteristic) curve](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)**:\n",
    "\n",
    "In the figure above, you can see the graphical representation of the trade-off between **True Positive Rate (Recall)** and the **[False Positive Rate](https://en.wikipedia.org/wiki/False_positive_rate)** of the model at different thresholds for the classification decision. The diagonal gray dashed line corresponds to a random classification with the same true positive rate and false positive rate.\n",
    "\n",
    "The **false positive rate** = (1 - **[Specificity](https://en.wikipedia.org/wiki/Sensitivity_and_specificity)**), as the following applies:\n",
    "\n",
    "$$FPR + \\text{Specificity} = \\frac{FP}{(FP + TN)} + \\frac{TN}{(FP + TN)} = \\frac{(FP + TN)}{(FP + TN)}= 1 $$\n",
    "\n",
    "$$FPR = 1 - \\text{Specificity}$$\n",
    "\n",
    "In other words, the more positive results are recorded, the more negative results are incorrectly classified as positive. Ideally, the classifier should have a high value for the recall and a low value for the **false positive rate**, which corresponds to a steep left-sided increase in **ROC**. In practice, the **ROC** can help to find a suitable threshold and assess the performance of a classification by selecting the tangent point of a tangent at $45^\\circ$ angle to the **ROC curve** as the optimal threshold of the classification <cite id=\"c62fn\"><a href=\"#zotero%7C16738657%2FWPIDC5X6\">(Bruce et al., 2021)</a></cite> (pp. 232-236).\n",
    "\n",
    "**[AUROC (Area Under ROC)](https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve)**:\n",
    "\n",
    "The **AUROC** corresponds to the area under the **ROC** and can assume values between $1$ (ideal classifier) and $0.5$ (worst classifier), since values under $0.5$ lie above the diagonal again by swapping the classification. The **AUROC** can be used as a quality measure when comparing multiple models <cite id=\"dejcm\"><a href=\"#zotero%7C16738657%2FWPIDC5X6\">(Bruce et al., 2021)</a></cite> (p. 234).\n",
    "\n",
    "**Metrics for regression:**\n",
    "\n",
    "<cite id=\"ph4pt\"><a href=\"#zotero%7C16738657%2FN6ILIT6K\">(Matzka, 2021)</a></cite> (pp. 156-166)\n",
    "\n",
    "**[Mean Squared Error (MSE)](https://en.wikipedia.org/wiki/Mean_squared_error)**:\n",
    "\n",
    "Average of the squared errors between the predicted and actual values, reacts sensitively to outliers as these are squared in the $MSE$.\n",
    "\n",
    "$$\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^n (Y_i - \\hat Y_i)^2 $$\n",
    "\n",
    "**[Root Mean Squared Error (RMSE)](https://en.wikipedia.org/wiki/Root-mean-square_deviation)**:\n",
    "\n",
    "Square root of the MSE. This makes it easier to interpret than the $MSE$, as it is available in the same units as the data.\n",
    "\n",
    "$$\\text{RMSE} = \\sqrt{\\text{MSE}} $$\n",
    "\n",
    "**[Mean Absolute Error (MAE)](https://en.wikipedia.org/wiki/Mean_absolute_error)**:\n",
    "\n",
    "Average of the absolute errors between the predicted and actual values, robust against outliers.\n",
    "\n",
    "$$\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^n |\\hat Y_i - Y_i| $$\n",
    "\n",
    "where $n$ is the number of predictions, $\\hat Y_i$ is the value of the prediction and $Y_i$ is the observed value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20ab74c-868f-4a5a-901e-461148022eea",
   "metadata": {},
   "source": [
    "**[R-squared (coefficient of determination)](https://en.wikipedia.org/wiki/Coefficient_of_determination)**: Proportion of the explained variance in the model compared to the total variance of the dependent variable.\n",
    "\n",
    "$$R^2 = \\frac{\\sum_i (\\hat y_i - \\bar y_i)^2 }{\\sum_i (y_i - \\bar y_i)^2} $$\n",
    "\n",
    "In detail, $\\hat y_i$ is the prediction of the regression model, $\\bar y_i$ is the mean of the $y_i$ values and $y_i$ is the observed values.\n",
    "\n",
    "**Metrics for clustering:**\n",
    "\n",
    "**[Silhouette Score](https://en.wikipedia.org/wiki/Silhouette_(clustering))**:\n",
    "\n",
    "Measure of cohesion within a cluster and how well it is separated from other clusters.\n",
    "\n",
    "$$\\begin{equation}\n",
    "    S(o)=\n",
    "    \\begin{cases}\n",
    "      0, & \\text{if o is the only element in A} \\\\\n",
    "      \\frac{dist(B,o)-dist(A,o)}{\\text{max}[dist(A,o) , dist(B,o)]}, & \\text{otherwise}\n",
    "    \\end{cases}\n",
    "  \\end{equation} $$\n",
    "where $dist(A,o)$ is the average distance to all other objects in $A$ and $dist(B,o)$ is the average distance to all objects in $B$.\n",
    "\n",
    "The **silhouette coefficient** is then calculated as the arithmetic mean over $S(o)$ of the individual clusters:\n",
    "\n",
    "$$s_C = \\frac{1}{n_C} \\sum_{o \\in C} S(o) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e692b56-1edb-4343-a94d-bb317e792ebd",
   "metadata": {},
   "source": [
    "## $k$-fold cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ea2f36-1742-484d-9949-992264176c5c",
   "metadata": {
    "citation-manager": {
     "citations": {
      "m95lo": [
       {
        "id": "16738657/QRTMKI7W",
        "source": "zotero"
       }
      ]
     }
    }
   },
   "source": [
    "The **[cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics))** <cite id=\"m95lo\"><a href=\"#zotero%7C16738657%2FQRTMKI7W\">(Richter, 2019)</a></cite> (pp. 19-20) is a validation method in which the training and test set are split first and then the training set is divided into $k$ subsets of the same size. These subsets are divided into $k$ subsets $T_1,...,T_k$ and a different subset $T_i$ is used for validation in each case, while the remaining $k-1$ subsets are used for training. Finally, the average of the $k$ validation results is calculated.\n",
    "\n",
    "The following figure shows a schematic structure of **cross-validation** for $k=3$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f34fdb6-5d2d-40e0-9648-6c19676c7b3c",
   "metadata": {},
   "source": [
    "<img src=\"./images/cross2_engl.png\" alt=\"drawing\" width=\"60%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d6c39b-47b1-4ad3-92fa-3f77c4db8cbc",
   "metadata": {},
   "source": [
    "The $k$-fold **cross-validation** method offers the advantage of a better model evaluation, as it divides the data set into training and test sets and the model is trained and tested several times with different data combinations. This provides a more realistic estimate of model performance and reduces the risk of **overfitting**. Cross-validation is therefore helpful in making a decision about which is the best model or hyperparameter.\n",
    "\n",
    "In addition, cross-validation allows for more efficient use of data in situations where the amount of data is limited, as all data in the training set can be used for both training and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349b60b4-f68a-464d-b5bc-2978382b7616",
   "metadata": {},
   "source": [
    "## Supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7fdc7d-bfd5-4951-9aeb-1de0226e8147",
   "metadata": {
    "citation-manager": {
     "citations": {
      "nzd29": [
       {
        "id": "16738657/N6ILIT6K",
        "source": "zotero"
       }
      ]
     }
    }
   },
   "source": [
    "In this section, we look at supervised learning. We have already discussed some of the methods used in supervised learning, such as linear, multiple and logistic regression, in the chapter Machine Learning with `scikit-learn`. At this point, we would like to discuss the $K$-nearest neighbor model (KNN) as a further method of supervised learning, discuss general possibilities for optimizing models such as **GridSearch** and **Cross-validation** and explain the idea behind supervised learning in more detail <cite id=\"nzd29\"><a href=\"#zotero%7C16738657%2FN6ILIT6K\">(Matzka, 2021)</a></cite> (pp. 99-166)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b6a322-01d1-43df-82a3-159bd862dc2c",
   "metadata": {},
   "source": [
    "Characteristically, supervised learning involves passing labeled data so that the model is able to make predictions based on generalization of the relationships previously observed from this training dataset. During the training process, it is necessary to evaluate (supervise) the predictions of the model to ensure that it generalizes well to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84112c4c-39c6-48ef-9b86-b821bfc2230a",
   "metadata": {},
   "source": [
    "The vector of input values $X$ is called the **feature vector** or **input**, the individual components are called **features**. The output vector ($Y$) is called the **target vector**.\n",
    "\n",
    "The basic problem in supervised learning is to find a relationship between $X$ and $Y$ in the form $$f(X) = Y $$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eaca5bc-9df2-4fb9-a2b2-0806cc3e4503",
   "metadata": {},
   "source": [
    "$X$ is usually a real-valued vector, i.e. $X \\in \\mathbb{R}^d$. For $Y$, either $Y \\in \\mathbb{R}$ or $Y \\in {1,\\cdots,K}$ and $K \\in \\mathbb{N}$ applies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956066c4-0621-45cf-9204-e7d2d082ca4f",
   "metadata": {},
   "source": [
    "<img src=\"./images/supervised_engl2.png\" alt=\"drawing\" width=\"80%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bae32bb-5143-42b7-ac30-29452a5a889a",
   "metadata": {},
   "source": [
    "Supervised learning is mainly used for two tasks:\n",
    "\n",
    "If $Y$ is present as $Y \\in \\mathbb{R}$, we speak of regression problems and for $Y \\in {1,\\cdots,K}$ of classification problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ed3442-909a-4b94-a016-0e70c650987d",
   "metadata": {},
   "source": [
    "<img src=\"./images/reg_vs_class.png\" alt=\"drawing\" width=\"60%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4cf606-62da-49dd-af07-55c84c52feab",
   "metadata": {},
   "source": [
    "Representatives of models with supervised learning include the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee9da83-8b93-4a48-805b-2fe547ed6e6c",
   "metadata": {},
   "source": [
    "**Linear models:**\n",
    "\n",
    "* *Linear regression*\n",
    "\n",
    "**Linear regression** is a method for predicting a continuous target variable based on one or more input variables. It models the relationship between the inputs and the target variable as a linear function.\n",
    "        \n",
    "* *Polynomial regression*\n",
    "\n",
    "**Polynomial regression** is an extension of linear regression in which a polynomial function is used to model the dependent variable in relation to the independent variable.\n",
    "        \n",
    "* *Logistic regression*\n",
    "\n",
    "In contrast to linear regression, **logistic regression** is used when the target variable is binary (for example, the classes $0$ and $1$). It models the probability of a particular class occurring and uses the sigmoid function, also known as the logistic function, to transform the continuous predictions of the linear model to the range between $0$ and $1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba2cd47-1333-4368-8256-ce620707aa44",
   "metadata": {},
   "source": [
    "**$K$ Nearest Neighbors (KNN):**\n",
    "\n",
    "* *Classification with KNN*\n",
    "        \n",
    "* *Regression with KNN*\n",
    "\n",
    "**KNN** is a method of classification or regression in which a point is classified or predicted based on the $K$ nearest neighbors in the feature space. The similarity (distance) between the data points is used.\n",
    "\n",
    "**Tree-based models for classification:**\n",
    "\n",
    "* *Decision trees*\n",
    "\n",
    "A **decision tree** is a hierarchical model that represents a decision in the form of a tree. It helps to make classifications or predictions by making a series of decisions based on features of the data.\n",
    "        \n",
    "* *Random Forests*\n",
    "\n",
    "**Random Forest** is an ensemble method that combines multiple decision trees. Each tree is trained on a random sample of the data and features. The predictions of the trees are averaged to obtain a robust prediction.\n",
    "        \n",
    "**Decision tree-based algorithms for regression:**\n",
    "\n",
    "* *Random forest regression*\n",
    "\n",
    "**Random forests** can also be used for regression. Numerical values are averaged over the individual decision trees of the random forest using an ensemble method.\n",
    "\n",
    "**Support Vector Machines (SVM)**:\n",
    "\n",
    "* *Classification with SVM*\n",
    "        \n",
    "* *Regression with SVM*\n",
    "\n",
    "**SVM** is a classification and regression method that uses a separation plane or hyperplane model to divide data into different classes. It aims to achieve a maximum margin between the classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d667aea-29e0-4194-b9ce-8834a8a19a82",
   "metadata": {},
   "source": [
    "In the following chapters we will deal with the models: **Decision Trees**, **Random Forest** and **Support Vector Machines**, while in this chapter we will first deal with the **$K$-nearest neighbor algorithm**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01dc4f1c-d227-4229-b06d-13eece8d50e6",
   "metadata": {},
   "source": [
    "### $K$-nearest neighbors (KNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29dd0d9-60b3-409c-bc71-595f5ef07741",
   "metadata": {
    "citation-manager": {
     "citations": {
      "e45nb": [
       {
        "id": "16738657/WPIDC5X6",
        "source": "zotero"
       }
      ]
     }
    }
   },
   "source": [
    "The general idea of the $K$-nearest-neighbors algorithm is based on the following steps:\n",
    "\n",
    "* Find $K$ observations with similar features in terms of their predictors, where the decision on similarity is made based on distance metrics.\n",
    "\n",
    "* For classification: Determine which category or class is predominant among the similar observations and assign this category to the new observation.\n",
    "\n",
    "* For regression: Determine the average value of the similar observations and assign this to the new observation <cite id=\"e45nb\"><a href=\"#zotero%7C16738657%2FWPIDC5X6\">(Bruce et al., 2021)</a></cite> (pp. 248-251)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32220c2e-0fd6-46ec-9495-582e587f87b6",
   "metadata": {},
   "source": [
    "It should be noted that how the **similarity** is measured (distance measure), how many **nearest neighbors** ($K$) are defined and how the features are **scaled** has an influence on the results of the prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b134f4bb-1889-422a-8f55-a20794f5231d",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ed8683-ba3f-401d-b966-37f80d49a1e1",
   "metadata": {
    "citation-manager": {
     "citations": {
      "3gftp": [
       {
        "id": "16738657/E3KUYES4",
        "source": "zotero"
       }
      ]
     }
    }
   },
   "source": [
    "As mentioned, the $K$ nearest neighbor algorithm uses metrics to measure the similarity of observations. In general, these **[metrics](https://en.wikipedia.org/wiki/Metric_space)** $d(x,y)$ can be defined on a set of elements if the following properties are fulfilled <cite id=\"3gftp\"><a href=\"#zotero%7C16738657%2FE3KUYES4\">(Lang &#38; Pucker, 2016)</a></cite> (p. 354):\n",
    "\n",
    "* Non-negativity: $d(x, y) \\ge 0 $\n",
    "\n",
    "* Uniqueness: $d(x, y) = 0 \\ $, if $x = y$ applies\n",
    "\n",
    "* Symmetry: $d(x, y) = d(y, x)$\n",
    "\n",
    "* **[Triangle inequality](https://en.wikipedia.org/wiki/Triangle_inequality)**: $d(x, z) \\le d(x, y) + d(y, z)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38c4b89-260e-42aa-88cb-3893d0901175",
   "metadata": {},
   "source": [
    "Closely linked to the concept of metric is the definition of **[norm](https://en.wikipedia.org/wiki/Euclidean_space#Euclidean_norm)**. The norm is used to assign a number to elements of a metric space that describes their size. In relation to vectors, this corresponds to the **length** of a vector. The norm must generally fulfill the following properties:\n",
    "\n",
    "* Non-negativity: $||x|| \\ge 0$\n",
    "\n",
    "* Uniqueness: $||x|| = 0 \\ $, if $x = 0$ applies\n",
    "\n",
    "* Scaling: $||\\lambda x|| = |\\lambda|||x|| \\ $, where $\\lambda$ is a scalar\n",
    "\n",
    "* Triangle inequality: $||x + y|| \\le ||x|| + ||y|||$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb48e039-2336-4f0d-b4b1-befa472fd4cb",
   "metadata": {
    "citation-manager": {
     "citations": {
      "lin6r": [
       {
        "id": "16738657/E3KUYES4",
        "source": "zotero"
       }
      ]
     }
    }
   },
   "source": [
    "It can be shown that the scalar product can be used to define a norm <cite id=\"lin6r\"><a href=\"#zotero%7C16738657%2FE3KUYES4\">(Lang &#38; Pucker, 2016)</a></cite> (p. 357). Applied to vectors, the norm corresponds to the square root of the scalar product of a vector with itself:\n",
    "\n",
    "$$|\\vec x| = \\sqrt{ \\vec x \\cdot \\vec x}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622851b7-9858-43e8-9777-3d167f632f16",
   "metadata": {
    "citation-manager": {
     "citations": {
      "z3awk": [
       {
        "id": "16738657/E3KUYES4",
        "source": "zotero"
       }
      ]
     }
    }
   },
   "source": [
    "Two of the most common distance measures that fulfill these relations are the following <cite id=\"z3awk\"><a href=\"#zotero%7C16738657%2FE3KUYES4\">(Lang &#38; Pucker, 2016)</a></cite> (p. 356):\n",
    "\n",
    "* Euclidean distance, which is defined as follows:\n",
    "\n",
    "Let $\\vec a$ and $\\vec b$ be two vectors in a $d$-dimensional space $\\vec a,\\vec b \\in \\mathbb{R}^d$, then the Euclidean distance between them is:\n",
    "\n",
    "$d(a_i,b_j) = \\sqrt{(a_1 - b_1)^2+(a_2 - b_2)^2+ \\cdots + (a_d - b_d)^2} $\n",
    "\n",
    "* The city block metric or **[Manhattan metric](https://en.wikipedia.org/wiki/Taxicab_geometry)** for which the distance between two vectors $\\vec a$ and $\\vec b$ is given by:\n",
    "\n",
    "$d(a_i,b_j) = |(a_1 - b_1)|+|(a_2 - b_2)|+ \\cdots + |(a_d - b_d)| = \\sum_d |a_d - b_d|$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8714093-0ae0-4421-b840-8a08e18ce402",
   "metadata": {
    "citation-manager": {
     "citations": {
      "9uwzo": [
       {
        "id": "16738657/E3KUYES4",
        "source": "zotero"
       }
      ],
      "lgtci": [
       {
        "id": "16738657/EPPNN3A6",
        "source": "zotero"
       }
      ]
     }
    }
   },
   "source": [
    "The terms **metric** and **norm** can also be defined much more generally (for example, to general vector spaces or function spaces). You can find out more about this in <cite id=\"9uwzo\"><a href=\"#zotero%7C16738657%2FE3KUYES4\">(Lang &#38; Pucker, 2016)</a></cite> (pp. 349-372) and <cite id=\"lgtci\"><a href=\"#zotero%7C16738657%2FEPPNN3A6\">(Lenze, 2020)</a></cite> (pp. 339-410)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2255a508-baee-4525-9bbc-6718f1522370",
   "metadata": {},
   "source": [
    "### Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb34a60-7871-440d-b5a3-620ce54645b3",
   "metadata": {},
   "source": [
    "#### $z$-Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207bfde9-d393-44d0-9e1e-3dc9fd129abd",
   "metadata": {
    "citation-manager": {
     "citations": {
      "qt0dd": [
       {
        "id": "16738657/WPIDC5X6",
        "source": "zotero"
       }
      ]
     }
    }
   },
   "source": [
    "For methods that compare similarities based on distance measures, it is important to pay attention to the scaling of the data. For example, **[$z$-standardization](https://en.wikipedia.org/wiki/Standard_score)** can be used to bring the predictor variables to the same scales <cite id=\"qt0dd\"><a href=\"#zotero%7C16738657%2FWPIDC5X6\">(Bruce et al., 2021)</a></cite> (pp. 254-257).\n",
    "\n",
    "The $z$-standardization is given by:\n",
    "\n",
    "$$ z = \\frac{x-\\bar x}{\\sigma} $$\n",
    "\n",
    "where $\\bar x$ corresponds to the mean value of the data and $\\sigma$ to the standard deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c126f980-af51-4c42-98d9-ee78c2014e95",
   "metadata": {},
   "source": [
    "#### Min-max scaling (normalization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c9cddd-d4ef-4586-98f3-7f0f64eb699a",
   "metadata": {},
   "source": [
    "Another common form of scaling is **[min-max scaling](https://en.wikipedia.org/wiki/Feature_scaling#Rescaling_(min-max_normalization))**, also known as normalization. Here, the characteristics under consideration are mapped to values between $[0 , 1]$ by subtracting the minimum characteristic value from all values and dividing by the difference between the maximum and minimum characteristic value:\n",
    "\n",
    "$$x^\\prime = \\frac{x - min(x)}{max(x) - min(x)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3af868-b3e5-42b6-8720-d199bf669960",
   "metadata": {},
   "source": [
    "To map the values to an arbitrary interval $[a , b]$, you can write the min-max scaling in the following form:\n",
    "\n",
    "$$x^\\prime = a + \\frac{(x - min(x))(b - a)}{max(x) - min(x)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b59733b-8176-4f7d-82d8-5a9313a60afe",
   "metadata": {},
   "source": [
    "In `scikit-learn`, various forms of scaling are contained in the `sklearn.preprocessing` module. For example, the $z$ standardization is implemented as `StandardScaler()` and the min-max scaling as `MinMaxScaler()`. The scaling is applied using the `fit_transform` method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4534b4eb-96dd-4a17-befe-f5a9271a80c5",
   "metadata": {},
   "source": [
    "In order to avoid potential **[data leakage](https://en.wikipedia.org/wiki/Leakage_(machine_learning))**, it is important to note that the `fit_transform()` method is **only** applied to the **training data** when scaling. The **test data** is scaled using the parameters determined on the training data with the `transform()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c94e043-1a0f-47ec-b3d1-ead162875c2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "740f66c5-4f27-4911-8265-8dcd5f5b818c",
   "metadata": {},
   "source": [
    "* Processing tabular data with `Pandas`\n",
    "\n",
    "* Learning the `Python` fundamentals necessary for machine learning\n",
    "\n",
    "* Linear, polynomial and logistic regression with `scikit learn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb19c7a-cdde-4157-939e-e2123c8b32b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5342cdb2-c090-4235-85b2-9dfbb73c5e50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca3f290-110e-4e54-a37a-63c9d7a02f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Startzeit des Trainings\n",
    "start_time = time.time()\n",
    "\n",
    "# Laden des MNIST-Datensatzes\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Vorverarbeitung der Daten\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "# Erstellen des Modells\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),          # Eingabeschicht\n",
    "    Dense(128, activation='relu'),          # Erste versteckte Schicht\n",
    "    Dense(64, activation='relu'),           # Zweite versteckte Schicht\n",
    "    Dense(10, activation='softmax')         # Ausgabeschicht\n",
    "])\n",
    "\n",
    "# Kompilieren des Modells\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Training des Modells (die Anzahl der Epochen und Batch-Größe kann angepasst werden)\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=64, validation_data=(x_test, y_test))\n",
    "\n",
    "# Testen des Modells\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2)\n",
    "print(f\"Testgenauigkeit: {test_acc}\")\n",
    "\n",
    "# Endzeit des Trainings\n",
    "end_time = time.time()\n",
    "\n",
    "# Berechnen der Trainingsdauer\n",
    "training_time = end_time - start_time\n",
    "print(f\"Training Dauer: {training_time:.2f} Sekunden\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae614546-302d-4873-8505-91ead2da538e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d243569c-699f-40ea-a9b6-105dc9b3b72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_search(element, list1):\n",
    "    count = 0\n",
    "    for number in list1:\n",
    "        count += 1\n",
    "        if number == element:\n",
    "            return print(f'Element: {number} found at {count} of the list.')\n",
    "    return print('Element not found!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2524fd61-12d1-4ed8-bf0c-07215ddc31e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [4, 1, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "329a286a-5efa-41d8-9cc3-4ff054184d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Element: 4 found at 1 of the list.\n"
     ]
    }
   ],
   "source": [
    "result = linear_search(4, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2e679b4-7b1e-49b8-a118-c8ec3680d56d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Element not found!\n"
     ]
    }
   ],
   "source": [
    "result = linear_search(5, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a5f2fd-206e-4457-8e6d-dc2b166f9e64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
